<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[DATa-ing]]></title>
  <link href="http://vikasrtr.github.io/atom.xml" rel="self"/>
  <link href="http://vikasrtr.github.io/"/>
  <updated>2015-09-07T11:54:36+05:30</updated>
  <id>http://vikasrtr.github.io/</id>
  <author>
    <name><![CDATA[Vikas Raturi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Science of A/B Testing]]></title>
    <link href="http://vikasrtr.github.io/science-of-a-b-testing/"/>
    <updated>2015-09-01T00:00:00+05:30</updated>
    <id>http://vikasrtr.github.io/science-of-a-b-testing</id>
    <content type="html"><![CDATA[<p>Modern day web or mobile ecosystem thrive on data-driven decisions, based on data obtained from <em>well-designed</em> <strong>experiments</strong>. Lets take look at the science, behind one of the most popular experiments on web - <strong>A/B Tests</strong>.</p>

<!--more-->


<p>This post is part of a series about A/B tests.
I will post the links as i write the subsequent posts.</p>

<script
    src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,http://vikasrtr.github.io/javascripts/MathJaxLocal.js'>
</script>


<h2>What is A/B testing</h2>

<blockquote><p>A/B testing or split testing is a randomized experiment, which compares performance of a metric, over alternate versions of same web page, simultaneously.</p></blockquote>

<p>Simply, in A/B testing, different users are shown different variants of same web page and performance of a given metric, say <em>clicking a button</em>, is compared. How we select the users and how measure such a performance is key to successful test.</p>

<h1></h1>

<p>To understand A/B test, lets break-through the definition one-by-one:</p>

<blockquote><p><strong>a randomized experiment</strong></p></blockquote>

<p>What is a randomized experiment and why do we need it ?<br/>
A randomized experiment refers to an experiment in which the subjects or objects are <code>selected</code> in randomized fashion. This is done, so that the process of selection represents a <code>chance model</code> and any <code>bias</code> is reduced.</p>

<blockquote><p><strong>performance of a metric</strong></p></blockquote>

<p>A metric connects the maths to business.<br/>
A metric can be any business query, that the test seeks to be answered. For example, <em>does the new search box works?</em> or <em>does changing color of Buy button increase sales?</em></p>

<blockquote><p><strong>alternate versions of same page</strong></p></blockquote>

<p>A/B test <strong>must</strong> always be done on <strong>same page</strong> with <strong>slight</strong> variation. The slight variation scheme is used to eliminate the <code>confounding</code> variable, which may hide the cause of a particular result or action.
However, contextualizing the results of a test may sometimes bring wonderfully surprising results, as mentioned here at <a href="http://nerds.airbnb.com/experiments-at-airbnb/">airbnb&rsquo;s blog</a>.</p>

<blockquote><p><strong>simultaneously</strong></p></blockquote>

<p>This perhaps may be the most important characterisitc of A/B tests. The test must be performed for all subjects at same time. This is done to create a notion of independence among subjects of test.<br/>
For example, <em>running part of test on Sunday or other on Monday, will lead to biased results</em>.</p>

<h2>The Basics</h2>

<p>In A/B testing, the users of application, are first randomly selected and placed into several buckets or groups and then tested for result. The process can be sumarized as:</p>

<ul>
<li>Categorize visitors into groups</li>
<li>Provide variants of a web page to users of each groups, with all users of same group getting same variant.</li>
<li>Measure and compare the response</li>
</ul>


<h2>The Maths</h2>

<p>As theory of probabilistic development goes with gamblers, the statistical theory goes with, <strong>medical field of clinical trials</strong> and thus it borrows much of its conventions from there. For simplicity let us assume that we want to generate only two groups.</p>

<blockquote><p>In an A/B test, the incoming users are randomly placed into two groups.<br/>
One group is shown the <strong>current web page</strong> and another is shown a variant.</p></blockquote>

<p>The users shown the current page are called - <strong>Control Group</strong> and the users, which gets the variant are called - <strong>Treatment Group</strong>.</p>

<blockquote><p>The resaon for using control group is to have a baseline for comparing the result of variant.</p></blockquote>

<p>Statistically, A/B tests are simply <code>two-sample hypothesis tests</code>.</p>

<p><strong>What is hypothesis testing?</strong><br/>
A hypothesis is simply <em>a statement</em>, that hasn&rsquo;t been proved true. And hypothesis testing is <em>process of verifying that, whether a given hypothesis is true</em>.</p>

<p>Since, a hypothesis can be complex enough to deduce its probability, we use a different approach to prove our hypothesis, which is known as <strong>null-hypothesis testing</strong>. In null-hypothesis testing, we use a baseline claim called - <code>null hypothesis</code> and <em>try to reject</em> it based on comparison to another hypothesis called - <code>alternate hypothesis</code>.</p>

<blockquote><p>In A/B testing, null-hypothesis states that &ldquo;the variant has no effect on user&rdquo;, while alternate hypothesis states that &ldquo;the variant has <strong>significant</strong> effect on user!&rdquo;</p></blockquote>

<p>Mathematically,<br/>
$$H_o = p_c - p_t$$
where $H_o$ is the null-hypothesis &amp; $p_c$ and $p_t$ are rate of change of metric for control and treatment groups respectively.</p>

<p>To reject the null-hypothesis, we have to prove that:<br/>
$$H_o = p_c - p_t &lt; 0 $$
i.e. performance of metric is less in treatment group, than control group.</p>

<p><strong>But why we need these hypotheses things ?</strong><br/>
One common understanding is to think that $p_c - p_t$ is simply the required difference, so why need a hypothesis testing.<br/>
The answer is that we want to account for <em>chance variation</em>. Since, even if both groups are shown <strong>exactly same page</strong> (something called <em>dummy testing</em>), even then the rates will be different due to chance variation. And we don&rsquo;t want to be misleaded by chance, making sure that the results are due to actual difference.</p>

<p><strong>How to be confident: Two sample Z-test</strong><br/>
To account for chance variation, we use little stats and something called Z-test.<br/>
Z-test falls in domain of significance testing, allowing us to detect the <em>significance</em> of results as compared to null-hypothesis.</p>

<p>Simply, Z-score is:<br/>
$$ Z = \frac{\text{difference of rates}}{\text{Standard Error of sample}}$$
And the complete equation thus becomes,<br/>
$$Z = \frac{p_c - p_t}{\sqrt{\frac{p_c(1 - p_c)}{N_c} + \frac{p_t(1 - p_t)}{N_t}}}$$
Here, $N_c$ and $N_t$ are number of users in control and treatment groups respectively.</p>

<p><strong>But what actually is z-score</strong></p>

<blockquote><p>Z-score is the probability of obtaining a test-statisitc, as extreme as the observed one, assuming the null-hypothesis to be true. It is denoted by <strong>P</strong> and called as <strong>P-value</strong>.</p></blockquote>

<p>If P &lt; 0.05, then we conclude that the null-hypothesis is rejected, making the observed difference <em>statistically significant</em>, otherwise the observed difference is just due to chance variation.</p>

<p>In upcoming post we take a look at some data and code to perform an A/B experiment.</p>

<p>Stay Curious !!</p>

<h2>References</h2>

<ul>
<li><a href="http://20bits.com/article/statistical-analysis-and-ab-testing">http://20bits.com/article/statistical-analysis-and-ab-testing</a></li>
<li><a href="http://nerds.airbnb.com/experiments-at-airbnb/">http://nerds.airbnb.com/experiments-at-airbnb/</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
